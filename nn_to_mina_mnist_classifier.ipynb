{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f82HkEsVf9X",
        "outputId": "02ff0b61-d2f2-4a8d-ae9d-65cfed560efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from scipy.ndimage import zoom\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing function\n",
        "def resize_images(images):\n",
        "    return np.array([zoom(image, 0.5) for image in images])\n",
        "\n",
        "# Resize\n",
        "x_train = resize_images(x_train)\n",
        "x_test = resize_images(x_test)\n",
        "\n",
        "# Then reshape\n",
        "x_train = x_train.reshape(60000, 14*14)\n",
        "x_test = x_test.reshape(10000, 14*14)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# normalize to range [0, 1]\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "metadata": {
        "id": "rXZmY3LHVuGk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(14*14,)),\n",
        "    keras.layers.Dense(10, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "1Fw9PXW2VvU2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl3JO5YbVxEY",
        "outputId": "e816f63b-7c6b-4243-ff72-e045e20aa4e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.8342 - accuracy: 0.7591 - val_loss: 0.4038 - val_accuracy: 0.8920\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3786 - accuracy: 0.8927 - val_loss: 0.3181 - val_accuracy: 0.9100\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3296 - accuracy: 0.9058 - val_loss: 0.2954 - val_accuracy: 0.9153\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3104 - accuracy: 0.9106 - val_loss: 0.2828 - val_accuracy: 0.9203\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2976 - accuracy: 0.9152 - val_loss: 0.2768 - val_accuracy: 0.9227\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2900 - accuracy: 0.9161 - val_loss: 0.2686 - val_accuracy: 0.9258\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2837 - accuracy: 0.9182 - val_loss: 0.2653 - val_accuracy: 0.9258\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2784 - accuracy: 0.9203 - val_loss: 0.2617 - val_accuracy: 0.9287\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2735 - accuracy: 0.9220 - val_loss: 0.2582 - val_accuracy: 0.9270\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2699 - accuracy: 0.9225 - val_loss: 0.2549 - val_accuracy: 0.9298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training the model, get weights for each layer\n",
        "weights_and_biases = [layer.get_weights() for layer in model.layers]\n",
        "\n",
        "# You can now print them or save them to a file\n",
        "# Here's how you could print them out\n",
        "for i, (weights, biases) in enumerate(weights_and_biases):\n",
        "    print(f\"Layer {i} weights:\\n{weights}\")\n",
        "    print(f\"Layer {i} biases:\\n{biases}\")\n",
        "    print()\n",
        "\n",
        "# Optionally, save the weights and biases to a file\n",
        "import json\n",
        "\n",
        "# Convert weights and biases to a serializable format\n",
        "weights_and_biases_json = []\n",
        "for weights, biases in weights_and_biases:\n",
        "    layer_data = {\n",
        "        \"weights\": weights.tolist(), # convert numpy array to list\n",
        "        \"biases\": biases.tolist(),\n",
        "        \"activation\": \"relu\"\n",
        "    }\n",
        "    weights_and_biases_json.append(layer_data)\n",
        "\n",
        "# Save as JSON\n",
        "with open('model_weights_biases.json', 'w') as f:\n",
        "    json.dump(weights_and_biases_json, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu9orhDRVy-p",
        "outputId": "1e90f612-5970-4abd-f316-3c9a1cd01841"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 weights:\n",
            "[[ 0.03240661 -0.07526675 -0.04668833 ...  0.10983798  0.0773706\n",
            "  -0.10907006]\n",
            " [ 0.00235596  0.04597434  0.13683379 ... -0.04797761  0.06619181\n",
            "   0.01015948]\n",
            " [ 0.05243987  0.1493347   0.00987034 ... -0.04480775 -0.00181711\n",
            "  -0.15156697]\n",
            " ...\n",
            " [ 0.02373633 -0.05589575 -0.02571973 ... -0.12246406 -0.01898363\n",
            "  -0.09637746]\n",
            " [-0.08516967  0.07546547  0.08525816 ...  0.08736441 -0.06885962\n",
            "  -0.03544681]\n",
            " [ 0.11327258  0.124293    0.05559546 ... -0.0150359  -0.15961856\n",
            "  -0.05813225]]\n",
            "Layer 0 biases:\n",
            "[ 0.01760766  0.11778699  0.67442447 -0.08673126  0.34683675  0.2402284\n",
            "  0.7554837   0.01920984 -0.1803472   0.25771418]\n",
            "\n",
            "Layer 1 weights:\n",
            "[[-0.8310304  -0.5480016  -0.17584494  1.0588604   0.2825471  -0.42427903\n",
            "  -1.642493    0.64329284 -0.03305861  0.40621683]\n",
            " [ 1.0183753   0.5691571  -0.9354249  -0.4954813  -0.8960686   0.33644572\n",
            "  -1.0138055   0.14725332  0.8587533  -0.04907818]\n",
            " [ 0.14975321  0.42267415 -0.94998926  0.55365497 -0.7261207   0.8231173\n",
            "  -0.42711416  0.9978937  -1.1520307  -0.35206252]\n",
            " [-0.6226813  -1.6124146  -1.1878389  -0.9387458   0.8130872  -0.5853827\n",
            "   1.054784    0.08497046  0.33719137  0.53787845]\n",
            " [-1.2821658   0.19935656  0.40934694 -0.35712373  0.10274798  0.93120396\n",
            "  -0.15646821 -0.7783467   0.43965834 -1.0129122 ]\n",
            " [-0.46082538  1.4227573  -0.10600188 -0.8474035  -0.25572038 -2.1950667\n",
            "   0.22558658 -0.07415146 -0.49862912  0.5625361 ]\n",
            " [ 0.5997061  -0.7383292   0.3713861  -1.4825208   0.962271    0.2432296\n",
            "  -0.29411143  0.16260876 -0.84242135  0.539637  ]\n",
            " [ 0.67296845 -1.0568161   1.0123533  -0.71734905 -1.1413243  -1.0026022\n",
            "   0.21680093  0.8463349  -0.79047    -0.7196992 ]\n",
            " [ 0.02707365 -0.79299396  0.4123105   0.46412772 -1.2067302   0.03683497\n",
            "  -0.00286265 -2.1817515   0.3343319  -0.02029042]\n",
            " [ 0.51667345  0.6308128   0.50450164  0.5813327   0.45005435 -0.08597545\n",
            "   0.78682363 -0.8235735  -0.33319268 -1.7602997 ]]\n",
            "Layer 1 biases:\n",
            "[-0.8118165   0.48205566  0.13674375  0.10219044 -0.22042036  1.0098488\n",
            " -0.19409318  0.38541153 -0.87633824 -0.01662266]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhfeVhGNV2Ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}